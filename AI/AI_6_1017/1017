# AI 파운데이션 및 개발 모델

### AI 파운데이션 모델의 개념

> AI 모델이 세상의 모든 데이터와 그 설명을 기억한다면?
> 
1. 파운데이션 모델
    
    : 대규모 데이터를 폭넓게 학습한 후, 다양한 문제에 빠르게 적응하는 범용 대형 AI 모델
    
    → 언어, 시각, 청각 등 기본적인 것들부터 배워나가는 기존 딥러닝 개발 패러다임에서
    
    → 거대 모델 + 대규모 데이터 학습 기반인 파운데이션 모델 패러다임으로
    
    - 개발 프로세스
        
        ![image.png](attachment:080ff5c7-31d8-4677-9ff8-c617bce7d74c:image.png)
        
    - 특징
        1. 대규모 : 트랜스포머 모델 + 대규머 언어 데이터 학습
        2. 적응성 : 높은 파인튜닝 성능
        3. 범용성 : 다양한 작업, 한정되지 않는 출력 지원
    
    > 이제는 잘 학습된 모델들을 얼마나 잘 활용하느냐가 핵심
    > 
    
    ![image.png](attachment:b3cba4da-1cd8-49a3-9363-023a323a7158:956e8047-77aa-4077-8212-4f91c81601c1.png)
    

### 대표적 AI 파운데이션 모델

1. CLIP
    
    > AGI 를 향해서 → LLM에 눈을 달아볼까? (시각언어모델)
    > 
    - 시각언어모델 예시
        - ChatGPT with GPT-4
        - Claude 기반 Computer use
    - 눈으로 어떤 것을 쓸까? → CLIP by OpenAI
    
    CLIP : Contrastive Language-Image Pre-training, by Open AI
    
    → AI가 언어와 시각을 통합해서 이해하는 방식을 보여준 패러다임 전환 제시
    
    ( 대조 학습 기반의 언어-이미지 사전 학습 )
    
    - 구조
        - 텍스트 인코더 ( Transformer 기반 Text Encoder )
        - 이미지 인코더 ( ViT : Vision Transformer )
    - 학습 (대조 학습)
        - 학습 기준
            - 목표 이미지(앵커)를 대응하는 텍스트(양성)와 가깝게
            - 일치하지 않는 여러 텍스트(음성)와는 멀게
            
            ![image.png](attachment:0c648bf9-f2ef-4a14-a878-ab39f8910ef5:image.png)
            
    - 간단 응용
        
        제로샷 이미지 인식기
        
        ![image.png](attachment:217412b7-5429-4dec-880f-5b690cd39372:image.png)
        

# Vision-Language Model(VML)

### CLIP (계속)

1. SigLIP (2023)
    
    softmax 대신 sigmoid 기반 손실함수
    
2. 멀티 모달 정합 응용
    
    : 서로 다른 두 가지 이상의 모달리티 간의 공통된 임베이딩 벡터 공간을 구성
    
    ![image.png](attachment:6e7f3e81-6138-4563-b074-be5074b4533e:image.png)
    
    - LLaVA ( Large language and Vision Assitant: 2023)
        
        : Vision과 Language모델을 결합한 모델(VLM)로, 텍스트와 이미지를 동시에 이해
        
        ![image.png](attachment:2d86c3f6-3fe7-4db4-aaf2-85de910deeda:image.png)
        
        - 특징
            - 효율적인 메모리 사용
            - 다중 모달 학습
            - Fine-tuning
        - 과정
            1. 사전학습
            2. Fine-tuning
            3. 시각 설명 데이터 생성
    
3. Vision_Language Models
    - 멀티모달 언어 모델
        
        ![image.png](attachment:767ffb8a-40d1-4906-a861-b4cfc5231fdb:image.png)
        
    - 최신 공개 VLM 모델들
        - Qwen-VL (Alibaba) : 여러 개의 이미지 입력, 번역, 텍스트 읽기, 인식
        - Qwen2-VL : 다국어 테스트 및 이미지 내 텍스트 이해 지원, 임의의 이미지 해상도 처리 가능
        - Qwen2.5-VL : 강력한 문서 파싱, 정밀한 객체 그라운딩, 장시간 비디오 이해
        - Qwen2.5-Omni : Omni = 전부
        - Qwen3-VL : 최근 공개
        - InternVl : 사용 VLM에 맞서는 오픈소스VLM
            
            ![image.png](attachment:079723a2-906d-4136-882a-60767861f83e:image.png)
            
    - 도메인 특화 파운데이션 모델들
        
        - Set of Mark(SoM) : 다른 물체 탐지, 세그멘테이션 파운데이션 모델을 활용한 방법
        
        - 의료
            - MedCLIP
            - LLaVA-Med
        - 제조업
            - AnomalyGPT
        - 3D 언어 모델
        - 로봇 행동 모델
        - 

# Small VLM과 파운데이션 모델들 소개

### Small Vision-Language Models(sVLM)

1. OpenVLM
2. sVLM
3. SmolVLM
4. Moodream 0.5B
5. Gemini Nano
6. 갤럭시 온디바이스 AI
7. LMDeploy → InternVL 배포
    
    ![image.png](attachment:c45034d1-d998-4aba-9612-745f2a359127:image.png)
    
8. 기타 sVLM

### 한국어 sVLM

1. 언어별 구조적/형태적 차이에 따른 토큰화 복잡성
    - 언어별 토큰 길이 격차 (토큰 정보밀도 차이)
        
        : 영어 중심 토크나이저 → 비영어권은 구조적 불이익
        
2. 한국어 sVLM 모델
    
    > Huggingface를 통해 한국어 sVLM 모델 사용
    > 
3. 다른 이미지 파운데이션 모델들 소개
    1. 이미지 파운데이션 모델
    2. 이미지 세그멘테이션 모델
        - Segment Anything - Meta
        - Grounding DINO - IDEA Research
        - SAMURAI - Univ.Washington ( 비디오 )
    3. 영상 생성 파운데이션 모델들
        - Closed 이미지 생성 모델
            - DALL E3(OpenAI)
            - Midjourney v7
        - Open Source 이미지 생성 모델
            - Stable Diffusion 3 / 3.5 (Stability AI)
            - FLUX (Black Forest Labs)
        - → 파인튜닝으로 용도 변경
            - ControlNet
            - 노블-뷰 생성 모델 - Zero123XL : 2D에서 3D
            - 정교한 3D Depth map 추정 : Marigold
            - 이미지 & 3D 동시 생성 모델 - JointDiT(Microsoft, POSTECH)
    4. 3D 파운데이션 모델
        - Depth Anything v2 ( HKU, TikTok)
        - 사람 중심 모델 - Sapiens(Meta)
        - Sora (OpenAI)
        - Veo 2 (Google Gemini)
        - Veo3
    5. Closed 비디어 생성 모델
        - 비디오 편집 모델
            - Modify Video (Luma Labs)
            - Canvas (Higgsfield)
        - HeyGen’s avatar IV
        - Wan 2.2
    6. Dynamic 3D 파운데이션 모델
        - MegaSaM(Google DeepMind)
        - CUT3R ( UC Berkeley, Google DeepMind)
    7. Audio-Vision Language Models

### 유용한 파운데이션 모델들 정리

![image.png](attachment:43343ff8-bc94-47bd-9b5c-1750751c1aac:image.png)

# 개인화, 합성 데이터 활용 사례

### 파운데이션 모델의 주 응용 방법 - 적응 학습 -

1. 파운데이션 + 미세조정(Fine-tuning)
    
    : 실용적인 개인화 파운데이션 모델
    
2. 미세조정 : 추가 학습을 통해 이미 학습된 모델을 조금만 튜닝하는 것
    
    → 특정 작업에 특화된 모델을 개발할 수 있다
    
    ![image.png](attachment:0cbfdf38-391a-4bb4-8687-8c651fe167f4:image.png)
    
3. 하이퍼파라미터 - Learning rate
    
    : 손실함수가 큰 값일 때 미세하게 조정하기 어려우므로 뉴럴넷 모델에 작은 비율(Learning rate)로 반영
    
4. Parameter-Efficient Fine-Tuning(PEET)
    - 프롬프트 디자인 : 입력 텍스트를 변형하는 방법
    - 프롬프트 튜닝 : 학습 가능한 프롬프트로서, 가상 토큰을 입력에 추가
        
        ![image.png](attachment:3c8723af-2c7b-4910-ae72-deb5679b345e:image.png)
        
    - Adaptor 모듈 추가 학습 : 작은 모듈을 추가하여 학습하는 기법
5. 개인화 모델 예시
    - DreamBooth : 영상 생성 모델 개인화 방법

### 데이터 효율화를 위한 합성 데이터 사용

1. 합성데이터 활용법
    - Knowlendge Distillation (Teacher-Student 학습)
        
        → 지식증류 : 높은 성능의 무거운 모델(선생님)을 모방하도록 가벼운 모델(학생)을 학습
        
        ![image.png](attachment:83f2a9c7-9ebe-40c0-b826-6e854477bdf4:image.png)
        
    - 파운데이션 모델들을 툴로 활용하는 방법 - InstructPix2pix
        
        : 명령(지시사항 : instruction)에 따라 이미지 편집을 수행하는 모델
        
        → 입/출력 이미지 상세 설명 없이, 명령만으로 편집 수행
        
    - 간단한 시뮬레이션 기반 합성 데이터 : 실제 데이터를 모방하거나 새로 생성한 인공 데이터
        
        : 실제 데이터를 수집하거나 사용하기 어려운 경우에 대체 가능
        
        → 데이터 부족 문제 해결, 모델 성능 개선에 사용
        

### AGI(**Artificial General Intelligence)**를 향해서

1. Sora : 텍스트 - 비디오 생성 모델
    
    ![image.png](attachment:44286905-0865-4982-a9d7-e3868c1fa29d:image.png)
    
2. 언어모델(LLM) + 검색증강생성(Retrieval Augmented Generation : RAG)
    
    > 유연성 : 추가 학습 없이 최신 정보 제공
    개인화 : 검색과 언어모델의 합성으로 개인 맞춤형 답변 가능
    정확성 : Verification을 통한 환각 현상 감소
    > 
    - Tool augmented LLM (Agent)
        - Visual Programming by AI
            
            ![image.png](attachment:28c95782-2853-4753-b204-2a230afb7210:image.png)
            
        - Toolformer
        - Claude - Computer Use, MCP
            
            : 텍스트를 기반으로 컴퓨터를 사람처럼 사용할 수 있는 서비스
            
    - 
3. Agent 모델
    - Genspart AI Browser
    - Agent Laboratory : Agent system 을 활용하여 연구를 자동으로 수행하는 시스템 개발
    - 

### 허깅페이스 (Huggingface)

 : AI관련 오픈소스 모델과 데이터셋을 공유하는 플랫폼

주요 특징 : 사전학습 모델 가중치 제공, 모델 학습을 위한 다양한 데이터셋 제공

응용 분야 : 자연어 처리(NLP), 컴퓨터 비전(CV), 음성 인식(Speech) 등

- 모델 서빙 : 사용자에게 모델의 예측 결과를 전달하는 절차 (허깅페이스에서 제공하는 모델 서빙)
- Hugging Face Inference API : 별도 서버 구축없이 플랫폼에서 REST API만으로 모델 바로 사용 가능
- 모바일 모델 서빙 : 모바일 서빙 프레임워크 MediaPipe, MLC LLM을 통해 쉽게 LLM 모델 실행 가능
    
    → 안드로이드에서 Gemma 실행하기
    
- 허깅페이스 기반 데모 페이지
    
     → Gradio : 머신러닝 모델을 웹 인터페이스로 쉽게 배포할 수 있게 도와주는 오픈소스 라이브러리
    

