# 생성데이터 기반의 ResNet 전이학습

생성데이터 기반의 ResNet 전이학습은 **부족한 실제 데이터를 보충하기 위해 GAN이나 확산 모델(Diffusion Model) 같은 생성 모델로 합성 데이터를 만든 뒤, 이 데이터를 ImageNet 등으로 사전 학습된 ResNet 모델에 적용하여 특정 작업을 수행하도록 미세 조정(fine-tuning)하는 기법**을 말합니다.

이는 데이터가 부족하거나, 민감한 정보(예: 의료, 개인정보)를 다뤄야 하거나, 클래스 불균형이 심각한 문제를 해결하기 위한 강력한 전략입니다.

---

## 💡 주요 목적 및 장점

크고 강력한 딥러닝 모델인 ResNet은 엄청난 양의 데이터를 필요로 합니다. 하지만 실제 현장에서는 양질의 데이터를 대량으로 수집하기 어려운 경우가 많습니다.

- **데이터 증강 (Data Augmentation):** 가장 큰 목적입니다. 수천, 수만 장의 실제 같은 합성 데이터를 생성하여 학습 데이터의 양을 획기적으로 늘릴 수 있습니다.
- **데이터 희소성 문제 해결:** 실제 수집이 거의 불가능한 희귀 사례(예: 드문 질병의 의료 영상, 특정 산업 설비의 고장 유형) 데이터를 생성하여 모델이 학습할 수 있게 합니다.
- **프라이버시 보호:** 원본 데이터의 통계적 분포는 학습하되, 개인을 식별할 수 없는 새로운 데이터를 생성하여 민감한 정보를 보호하며 모델을 개발할 수 있습니다.
- **클래스 불균형 해소:** 데이터가 적은 소수 클래스(minority class)의 데이터만 집중적으로 생성하여 데이터셋의 균형을 맞출 수 있습니다.

---

## 🔄 작동 프로세스

전체 과정은 크게 **'생성 모델 학습'**과 **'ResNet 전이학습'** 두 단계로 나뉩니다.

1. **사전 학습된 ResNet 준비:**
    - 먼저 ImageNet과 같은 대규모 데이터셋으로 미리 학습된 **ResNet 모델(예: ResNet-50)**을 불러옵니다. 이 모델은 이미지의 기본적인 특징(선, 질감, 형태 등)을 이미 잘 파악하고 있습니다.
2. **생성 모델 학습 (데이터 생성 단계):**
    - 보유하고 있는 **소량의 실제 데이터**를 사용하여 GAN(Generative Adversarial Network)이나 VAE, 확산 모델과 같은 생성 모델을 학습시킵니다.
    - 이 생성 모델은 실제 데이터의 분포를 학습하여, 원본과 매우 유사하지만 실제로는 존재하지 않는 **새로운 합성 데이터(Generated Data)**를 만들어냅니다.
3. **데이터셋 구성:**
    - 학습에 사용할 최종 데이터셋을 구성합니다.
    - **방법 A:** 생성된 데이터만을 사용 (원본 데이터 접근이 불가능할 경우)
    - **방법 B:** 기존의 소량 실제 데이터 + 생성된 합성 데이터를 함께 사용 (가장 일반적이며 성능이 좋음)
4. **ResNet 전이학습 (미세 조정 단계):**
    - 준비된 ResNet 모델의 마지막 분류 레이어(ImageNet 1000개 클래스용)를 **현재 작업에 맞는 새로운 레이어** (예: '정상/불량' 2개 클래스용)로 교체합니다.
    - 구성한 데이터셋(실제+합성 데이터)을 사용하여 이 ResNet 모델을 **미세 조정(fine-tuning)**합니다.
    - 이때, 모델의 하위 레이어(특징 추출부)는 가중치를 '동결(freeze)'하거나 아주 작은 학습률로 학습시키고, 새로 추가된 상위 레이어(분류기)를 집중적으로 학습시킵니다.

---

## ⚠️ 주요 고려사항

- **생성 데이터의 품질:** 생성된 데이터의 품질이 낮으면 오히려 모델 성능이 저하될 수 있습니다 ("Garbage in, garbage out"). 실제 데이터와 구별이 어려울 정도로 품질이 높아야 합니다.
- **다양성 (Mode Collapse):** 생성 모델이 비슷비슷한 이미지만 만들어내는 '모드 붕괴' 현상이 발생하면 데이터 증강 효과가 떨어집니다. 생성된 데이터가 충분히 다양한지 확인해야 합니다.
- **현실과의 차이 (Domain Gap):** 아무리 잘 만든 합성 데이터라도 실제 데이터의 미묘한 노이즈나 특성을 100% 반영하기는 어렵습니다. 합성 데이터로만 학습할 경우 실제 데이터를 만났을 때 성능이 떨어질 수 있어, 가급적 실제 데이터와 혼합 사용하는 것이 좋습니다.

---

# 자연어처리 및 텍스트 파운데이션 모델 (LLM)

1. **파운데이션 모델**
    
    : 대량의 데이터를 기반으로 사전 학습된 대규모 AI 모델
    
    - 구성요소
        1. 빅데이터 : 인터넷에 존재하는 데이터 수가 기하급수적으로 증가
        2. 자가 학습 알고리즘 : 사람이 정답을 알려줄 필요 없음
        3. 어텐션 기반 트랜스포머 모델 : 더 많은 데이터를 학습할 수 있는 인공신경망 구조
    
2. **텍스트 파운데이션 모델( 거대 언어 모델 )**
    
    특이점
    
    1. 규모의 법칙 : 더 많은 데이터, 큰 모델, 긴 학습 → 더 좋은 성능
    2. 창발성 : 특정 규모를 넘어서면 갑자기 모델에서 발현되는 성질
    
    > 기존 대비, 더 큰 모델이 더 많은 데이터에서 학습되어 창발성이 나타나기 시작한 언어 모델
    > 
    > 
    > → GPT처럼 다음 토큰 예측을 통해 많은 텍스트 데이터에서 사전 학습된 트랜스포머 기반 모델
    > 
    
    - 폐쇄형 거대 언어 모델
        
        ex) ChatGPT, Claude, Gemini
        
        ![image.png](attachment:9f12593e-95ce-49a6-8738-0b72bd32abdd:image.png)
        
    - 개방형 거대 언어 모델
        
        ex) LLaMA, Gemma, Qwen
        
        장점 : 무료, 모든 정보, 모델 구조, 소스코드가 공개되어 있음
        
        단점 : 충분한 계산 지원 필요, 상대적으로 폐쇄형에 비해 성능 낮은 편
        

# **거대 언어 모델의 학습**

GPT-3 ( 거대 언어 모델의 시초 )

: 가장 큰 버전은 1750억개의 매개변수 → 이전 모델 대비 10배 큼

본격적으로 in-context learning 능력이 나타나기 시작한 언어 모델

- 학습 방법 : 다음 토큰 예측
- 학습 데이터 3000억 토큰
- 학습 비용 : 150억원  추산

정렬 학습

1. 지시 학습 : 주어진 지시에 어떤 응답이 생성되어야 하는지
2. 선호 학습 : 상대적으로 어떤 응답이 선호되어야 하는지

1. **지시 학습 ( Instruction tunint )**
    
    : 거대 언어 모델을 다양한 지시 기반 입력과 이에 대한 응답으로 추가 학습 (FLAN)
    
    ![image.png](attachment:1cc8ef0d-25c4-4488-9267-56665fd2793e:image.png)
    
2. **선호 학습 ( Preference learning )** 
    
    > 정답이 정해져 있지 않은 개방형 테스크에서는 한계가 있음
    > 
    
    : 다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가 학습 ( 선호도는 사람이 제공 )
    
    ![image.png](attachment:abca3d36-7ca3-4ad2-9015-c667a9abd7ae:image.png)
    
    - InstructGPT
        
        : 사람의 피드백을 통한 강화학습(Reinforcement Learning form Human Feedback, RLHF)
        
        과정
        
        1. 지시 학습을 텍스트 파운데이션 모델의 추가 학습
        2. 사람의 선호 데이터를 수집하여, 보상 모델을 학습
            
            ![image.png](attachment:3a1f2fc1-90d6-4ed8-9942-03b1c45a0867:image.png)
            
        3. 보상이 높은 응답을 생성하도록 강화 학습을 통해 추가 학습
        4. 유저의 지시를 얼마나 잘 수행하는지, 얼마나 안전한 응답을 생성하는지 사람이 평가
    - LLaMA2
        
        : InstructGPT처럼 RLHF와 대화데이터를 활용 → 당시 대화형 타입 Open LLM 중 가장 우수
        

# **거대 언어 모델의 추론**

1. **디코딩 알고리즘**
    
    : 거대 언어 모델의 자동회귀 생성 ( Auto-regressive Generation )
    
    > 학습이 완료된 LLM은 순차적 추론을 통한 “토큰별 생성”으로 응답한다
    > 
    
    언제? ⇒ EOS 토큰 생성 시 종료 or 사전에 정의된 토큰 수 도달 시 종료
    
    Goal : 주어진 입력 x = [x^^1, …, x^^L] 에 대해 다음 토큰 x^^L+1 을 생성
    
    디코딩 알고리즘 :  (hat)p(x)로부터 x^^L+1을 생성하는 알고리즘 ( 다음 단어를 선택하는 방법 )
    
    ![image.png](attachment:d03ff202-a7bc-44eb-b3af-fbd6886e3461:image.png)
    
    1. Greedy Decoding : 가장 확률이 높은 다음 토큰을 선택
        
        ![image.png](attachment:1f54baa2-8ae5-4af8-a8b8-0661f6e0aa90:image.png)
        
    2. Beam Search : 확률이 높은 k개(beam size)의 후보를 동시에 고려
        
        ( 전체 문장 후보들의 누적 확률을 기준으로 상위 k개를 남기는 것 )
        
    3. Sampling : LLM이 제공한 확률을 기준으로 랜덤하게 생성
        
        ![image.png](attachment:d1f55323-ccc0-41b7-8ab7-4ec09c27921f:image.png)
        
    4. Sampling“with Temperature”: 하이퍼파라미터T를 통해 LLM이 생성한 확률분포를 임의로 조작
        
        ![image.png](attachment:0b654d76-1ee2-4876-bd17-6c8c650423b8:image.png)
        
    5. Top-K Sampling : 확률이 높은 K개의 토큰들 중에서만 랜덤하게 확률에 따라 샘플링
        
        장점 : 품질 낮은 응답 생성 가능성 줄일수있음
        
        단점 : 확률 분포의 모양에 상관 없이 고정된 k개의 후보군을 고려
        
    6. Top-P Sampling(Nucleus Sampling): K를 고정하는 대신 누적 확률P에 집중하여 K를 자동 조절
        
        ![image.png](attachment:d99d6f42-2559-44a9-808c-ffdf2f179e6c:image.png)
        
    
    ![image.png](attachment:b2b66f0b-646e-46ec-a219-f5a15dd12e2d:image.png)
    
2. **프롬프트 엔지니어링**
    
    : 원하는 답을 얻기 위해 모델에 주어지는 입력(프롬프트)을 설계/조정하는 기법
    
    입력 프롬프트 = 지시 ( Instruction ) + 예시
    
    - Chain-of-Thought (CoT) 프롬프팅
        
        : 단순히 질문과 응답만으로 예시 활용이 아닌, 추론 과정도 예시에 포함
        
        → CoT로 인한 성능 향상은 모델 크기가 커질 수록 더 확대됨 (추론 ~= 창발성?)
        
        but, 예시와 추론 과정을 수집해야한다는 문제점
        
        > ↓ 예시 없이도 추론 성능 강화하려면?
        > 
    - 0-shot CoT 프롬프팅
        1. 유인 문장을 통한 추론 생성 ( Let’s think step by step )
        2. 주어진 질문과 생성된 추론을 통한 정답 생성 ( Therefore, the answer is )

# 거대 언어 모델의 평가와 응용

1. **거대 언어 모델의 평가**
    - 평가 : 구축한 시스템이 실제로 잘 동작하는지를 확인하는 단계
        
        ![image.png](attachment:7014fa9c-dd1b-40b0-894b-7a8e49fbcb53:image.png)
        
    - AI 모델의 평가 : 테스트 데이터
        
        핵심 가정 : 학습 단계에서 본 적이 없고, 질문과 정답을 알고있음
        
    - 특징 : 특정 테스크에서 학습된 기존 AI모델들과 달리, LLM은 다양한 테스크에 대해 동시에 학습
    - 평가 방법의 종류
        - 정답이 정해진 경우 : 예측과 정답을 비교하여 일치도를 측정 → “정확도”
        - 정답이 정해져 있지 않은 경우
            1. 사람이 임의의 정답을 작성 및 이와 예측을 비교
            2. 정답과 무관하게 생성 텍스트 자체의 품질만을 측정
                
                → LLM을 활용한 평가(LLM-as-judge) : 거대 언어 모델을 통해 생성 텍스트를 평가
                
            3. 생성 텍스트의 “상대적 선호”를 평가할 수도 있음 ( ex - LMArena )
                
                → LLM-as-judge 로 상대적 선호도 평가 가능
                
                ![image.png](attachment:2474eed5-c24d-41b2-94b6-9ab2262a7924:image.png)
                
    
2. **거대 언어 모델의 응용 및 한계**
    
    **응용**
    
    - 멀티모달 파운데이션 모델
        
        : 멀티모달(이미지, 비디오, 오디오) 입력 → 멀티모달(오디오, 텍스트) 출력
        
        : 다른 모달리티 데이터를 LLM이 이해할 수 있도록 토큰화 및 추가 학습
        
    - 합성 데이터 생성 :
        - Self-instruct: 175개의 데이터를 사람이 작성뒤, GPT-3를 통해 52000개의 합성 데이터 생성
        - Alpagasus : 프롬프팅을 통한 합성 데이터의 품질 평가 및 필터링 제안
    
    **한계**
    
    - 환각 (Hallucination)
        
        : 사실과 다르거나 전적으로 지어낸 내용임에도, 정확한 정보와 동일한 자신감과 유창함으로 응답
        
    - 탈옥 ( Jailbreaking)
        
        : 프롬프팅 엔지니어링을 통해 LLM의 정렬을 우회할 수 있다는 것이 확인됨
        
    - AI 텍스트 검출
        
        : LLM의 무분별한 사용의 파생적 문제